Index: vm_core.h
===================================================================
--- vm_core.h	(リビジョン 16421)
+++ vm_core.h	(作業コピー)
@@ -449,6 +449,14 @@
 #endif
     jmp_buf machine_regs;
     int mark_stack_len;
+    //lazy sweep    
+    size_t *local_slots;
+    VALUE free_cache;
+    size_t sweep_index;
+    size_t slots_length;
+    size_t slots_used;
+    size_t do_heap_free;
+    size_t freed;
 
     /* statistics data for profiler */
     VALUE stat_insn_usage;
Index: gc.c
===================================================================
--- gc.c	(リビジョン 16421)
+++ gc.c	(作業コピー)
@@ -123,20 +123,14 @@
     char *file;
     int   line;
 #endif
+    char color;
 } RVALUE;
 
 #if defined(_MSC_VER) || defined(__BORLANDC__) || defined(__CYGWIN__)
 #pragma pack(pop)
 #endif
 
-struct heaps_slot {
-    void *membase;
-    RVALUE *slot;
-    int limit;
-};
-
 #define HEAP_MIN_SLOTS 10000
-#define FREE_MIN  4096
 
 struct gc_list {
     VALUE *varptr;
@@ -153,13 +147,16 @@
 	struct heaps_slot *ptr;
 	size_t length;
 	size_t used;
-	RVALUE *freelist;
 	RVALUE *range[2];
 	RVALUE *freed;
+	size_t live;
+	size_t sweep_index;
+	size_t sweep_increment;
     } heap;
     struct {
 	int dont_gc;
 	int during_gc;
+	int during_sweep;
     } flags;
     struct {
 	int need_call;
@@ -186,13 +183,16 @@
 #define heaps			objspace->heap.ptr
 #define heaps_length		objspace->heap.length
 #define heaps_used		objspace->heap.used
-#define freelist		objspace->heap.freelist
 #define lomem			objspace->heap.range[0]
 #define himem			objspace->heap.range[1]
 #define heaps_inc		objspace->heap.increment
 #define heaps_freed		objspace->heap.freed
+#define live                    objspace->heap.live
+#define heaps_sweep_index       objspace->heap.sweep_index
+#define heaps_sweep_inc         objspace->heap.sweep_increment
 #define dont_gc 		objspace->flags.dont_gc
 #define during_gc		objspace->flags.during_gc
+#define during_sweep            objspace->flags.during_sweep
 #define need_call_final 	objspace->final.need_call
 #define finalizer_table 	objspace->final.table
 #define deferred_final_list	objspace->final.deferred
@@ -316,11 +316,11 @@
     if (size == 0) size = 1;
 
     if (ruby_gc_stress || (malloc_increase+size) > malloc_limit) {
-	garbage_collect(objspace);
+	garbage_collect_force(objspace);
     }
     RUBY_CRITICAL(mem = malloc(size));
     if (!mem) {
-	if (garbage_collect(objspace)) {
+	if (garbage_collect_force(objspace)) {
 	    RUBY_CRITICAL(mem = malloc(size));
 	}
 	if (!mem) {
@@ -381,10 +381,10 @@
     }
     if (!ptr) return ruby_xmalloc(size);
     if (size == 0) size = 1;
-    if (ruby_gc_stress) garbage_collect(objspace);
+    if (ruby_gc_stress) garbage_collect_force(objspace);
     RUBY_CRITICAL(mem = realloc(ptr, size));
     if (!mem) {
-	if (garbage_collect(objspace)) {
+	if (garbage_collect_force(objspace)) {
 	    RUBY_CRITICAL(mem = realloc(ptr, size));
 	}
 	if (!mem) {
@@ -536,12 +536,61 @@
     heaps_length = next_heaps_length;
 }
 
+//lazy sweep
 static void
+allocate_slots_thread(rb_objspace_t *objspace, rb_thread_t *th) {
+    size_t *p;
+    size_t size;
+
+    size = heaps_length*sizeof(th->local_slots);
+    RUBY_CRITICAL(
+		  if (th->slots_used > 0) {
+		      p = (size_t *)realloc(th->local_slots, size);
+		      if (p) th->local_slots = p;
+		  }
+		  else {
+		      p = th->local_slots = (size_t *)malloc(size);
+		  }
+		  );
+    if (p == 0) rb_memerror();
+    th->slots_length = heaps_length;
+}
+
+#define thread_sweep_slot(th) (&heaps[th->local_slots[th->sweep_index]])
+#define RANY(o) ((RVALUE*)(o))
+
+static void
+assign_thread_local_slot(rb_objspace_t *objspace, rb_thread_t *th, size_t index, RVALUE *p, RVALUE *pend) {
+    size_t i;
+
+    if (th->slots_length < heaps_length) {
+	allocate_slots_thread(objspace, th);
+    }
+    for (i = 0; i < th->slots_used; i++) {
+	if (th->local_slots[i] >= index) th->local_slots[i]++;
+    }
+    MEMMOVE((&th->local_slots[0])+1, &th->local_slots[0], VALUE, th->slots_used);
+
+    while (p < pend) {
+	p->color = WHITE;
+	p->as.free.flags = 0;
+	p->as.free.next = RANY(th->free_cache);
+	th->free_cache = (VALUE)p;
+	p++;
+    }
+    th->local_slots[0] = index;
+    th->sweep_index++;
+    th->slots_used++;
+    th->do_heap_free = (th->slots_used * HEAP_OBJ_LIMIT) * 0.65;
+}
+
+static void
 assign_heap_slot(rb_objspace_t *objspace)
 {
     RVALUE *p, *pend, *membase;
-    size_t hi, lo, mid;
+    size_t hi, lo, mid, i;
     int objs;
+    rb_thread_t *th = GET_THREAD();
 	
     objs = HEAP_OBJ_LIMIT;
     RUBY_CRITICAL(p = (RVALUE*)malloc(HEAP_SIZE));
@@ -570,6 +619,7 @@
 	    hi = mid;
 	}
 	else {
+	    printf("membase = %p : mid_membase = %p\n", membase, mid_membase);
 	    rb_bug("same heap slot is allocated: %p at %ld", membase, mid);
 	}
     }
@@ -577,19 +627,15 @@
 	MEMMOVE(&heaps[hi+1], &heaps[hi], struct heaps_slot, heaps_used - hi);
     }
     heaps[hi].membase = membase;
-    heaps[hi].slot = p;
+    heaps[hi].slot = (VALUE)p;
     heaps[hi].limit = objs;
+    heaps[hi].color = BLACK;
+    heaps[hi].thread = (VALUE)GET_THREAD();
     pend = p + objs;
     if (lomem == 0 || lomem > p) lomem = p;
     if (himem < pend) himem = pend;
     heaps_used++;
-
-    while (p < pend) {
-	p->as.free.flags = 0;
-	p->as.free.next = freelist;
-	freelist = p;
-	p++;
-    }
+    assign_thread_local_slot(objspace, th, hi, p, pend);
 }
 
 static void
@@ -632,23 +678,25 @@
     return Qfalse;
 }
 
-#define RANY(o) ((RVALUE*)(o))
-
-static VALUE
+VALUE
 rb_newobj_from_heap(rb_objspace_t *objspace)
 {
     VALUE obj;
+    rb_thread_t *th = GET_THREAD();
 	
-    if (ruby_gc_stress || !freelist) {
-    	if (!heaps_increment(objspace) && !garbage_collect(objspace)) {
+    if (ruby_gc_stress || !th->free_cache) {
+   	if (!garbage_collect(objspace)) {
 	    rb_memerror();
 	}
     }
 
-    obj = (VALUE)freelist;
-    freelist = freelist->as.free.next;
+    obj = (VALUE)th->free_cache;
+    th->free_cache = (VALUE)(RANY(obj)->as.free.next);
 
-    MEMZERO((void*)obj, RVALUE, 1);
+    /* flags keep */
+    char c = RANY(obj)->color;
+     MEMZERO((void*)obj, RVALUE, 1);
+    RANY(obj)->color = c;
 #ifdef GC_DEBUG
     RANY(obj)->file = rb_sourcefile();
     RANY(obj)->line = rb_sourceline();
@@ -671,6 +719,7 @@
 
 	th->value_cache[i] = v;
 	RBASIC(v)->flags = FL_MARK;
+	RANY(v)->color = BLACK;
     }
     th->value_cache_ptr = &th->value_cache[0];
     rv = rb_newobj_from_heap(objspace);
@@ -693,6 +742,7 @@
 
     if (v) {
 	RBASIC(v)->flags = 0;
+	RANY(v)->color = WHITE;
 	th->value_cache_ptr++;
     }
     else {
@@ -822,10 +872,10 @@
 
     init_mark_stack(objspace);
     for (i = 0; i < heaps_used; i++) {
-	p = heaps[i].slot; pend = p + heaps[i].limit;
+	p = RANY(heaps[i].slot); pend = p + heaps[i].limit;
 	while (p < pend) {
-	    if ((p->as.basic.flags & FL_MARK) &&
-		(p->as.basic.flags != FL_MARK)) {
+	    if ((p->color & BLACK) &&
+		(p->as.free.flags != FL_MARK)) {
 		gc_mark_children(objspace, (VALUE)p, 0);
 	    }
 	    p++;
@@ -865,8 +915,8 @@
     while (lo < hi) {
 	mid = (lo + hi) / 2;
 	heap = &heaps[mid];
-	if (heap->slot <= p) {
-	    if (p < heap->slot + heap->limit)
+	if (RANY(heap->slot) <= p) {
+	    if (p < RANY(heap->slot) + heap->limit)
 		return Qtrue;
 	    lo = mid + 1;
 	}
@@ -1005,8 +1055,9 @@
     obj = RANY(ptr);
     if (rb_special_const_p(ptr)) return; /* special const not marked */
     if (obj->as.basic.flags == 0) return;       /* free cell */
-    if (obj->as.basic.flags & FL_MARK) return;  /* already marked */
-    obj->as.basic.flags |= FL_MARK;
+    if (obj->color & BLACK) return;  /* already marked */
+    obj->color = BLACK;
+    live++;
 
     if (lev > GC_LEVEL_MAX || (lev == 0 && ruby_stack_check())) {
 	if (!mark_stack_overflow) {
@@ -1039,9 +1090,10 @@
   again:
     obj = RANY(ptr);
     if (rb_special_const_p(ptr)) return; /* special const not marked */
-    if (obj->as.basic.flags == 0) return;       /* free cell */
-    if (obj->as.basic.flags & FL_MARK) return;  /* already marked */
-    obj->as.basic.flags |= FL_MARK;
+    if (obj->as.free.flags == 0) return;       /* free cell */
+    if (obj->color & BLACK) return;  /* already marked */
+    obj->color = BLACK;
+    live++;
 
   marking:
     if (FL_TEST(obj, FL_EXIVAR)) {
@@ -1301,137 +1353,129 @@
 static void
 finalize_list(rb_objspace_t *objspace, RVALUE *p)
 {
+    rb_thread_t *th = GET_THREAD();
+    
     while (p) {
 	RVALUE *tmp = p->as.free.next;
 	run_final(objspace, (VALUE)p);
 	if (!FL_TEST(p, FL_SINGLETON)) { /* not freeing page */
             VALGRIND_MAKE_MEM_UNDEFINED((void*)p, sizeof(RVALUE));
 	    p->as.free.flags = 0;
-	    p->as.free.next = freelist;
-	    freelist = p;
 	}
 	p = tmp;
     }
 }
 
-static void
-free_unused_heaps(rb_objspace_t *objspace)
-{
-    size_t i, j;
-    RVALUE *last = 0;
+void rb_gc_abort_threads(void);
 
-    for (i = j = 1; j < heaps_used; i++) {
-	if (heaps[i].limit == 0) {
-	    if (!last) {
-		last = heaps[i].membase;
+int
+slot_sweep(rb_objspace_t *objspace, struct heaps_slot *target) {
+    rb_thread_t *th;
+    RVALUE *p, *pend;
+    RVALUE *final;
+    VALUE freelist;
+    int freed = 0;
+
+    if (target->color == BLACK || target->color == WHITE || !target->limit) {
+	return Qfalse;
+    }
+
+    th = (rb_thread_t *)target->thread;
+    final = deferred_final_list;
+    freelist = th->free_cache;
+    p = RANY(target->slot); pend = p + target->limit;
+    while (p < pend) {
+	if (p->color != BLACK) {
+	    if (p->as.basic.flags) {
+		obj_free(objspace, (VALUE)p);
 	    }
+	    if (need_call_final && FL_TEST(p, FL_FINALIZE)) {
+		p->as.free.flags = FL_MARK; /* remain marked */
+		p->as.free.next = deferred_final_list;
+		deferred_final_list = p;
+	    }
 	    else {
-		free(heaps[i].membase);
+		VALGRIND_MAKE_MEM_UNDEFINED((void*)p, sizeof(RVALUE));
+		p->color = WHITE;
+		p->as.free.flags = 0;
+		p->as.free.next = RANY(th->free_cache);
+		th->free_cache = (VALUE)p;
 	    }
-	    heaps_used--;
+	    freed++;
 	}
+	else if (RBASIC(p)->flags == FL_MARK) {
+	    /* objects to be finalized */
+	    /* do nothing remain marked */
+	}
 	else {
-	    if (i != j) {
-		heaps[j] = heaps[i];
-	    }
-	    j++;
+	    p->color = WHITE;
 	}
+	p++;
     }
-    if (last) {
-	if (last < heaps_freed) {
-	    free(heaps_freed);
-	    heaps_freed = last;
+    if (freed == target->limit && th->freed > th->do_heap_free) {
+	RVALUE *pp;
+	
+	target->limit = 0;
+	for (pp = deferred_final_list; pp != final; pp = pp->as.free.next) {
+	    pp->as.free.flags |= FL_SINGLETON; /* freeing page mark */
 	}
-	else {
-	    free(last);
-	}
+	th->free_cache = freelist;	/* cancel this page from free_cache */
     }
+    else {
+	target->color = BLACK;
+	th->freed += freed;
+    }
+    return Qtrue;
 }
 
-void rb_gc_abort_threads(void);
+void
+heap_sweep_increment(rb_objspace_t *objspace) {
+    int i = 0;
 
-static void
-gc_sweep(rb_objspace_t *objspace)
-{
-    RVALUE *p, *pend, *final_list;
-    size_t freed = 0;
-    size_t i;
-    size_t live = 0, free_min = 0, do_heap_free = 0;
-
-    do_heap_free = (heaps_used * HEAP_OBJ_LIMIT) * 0.65;
-    free_min = (heaps_used * HEAP_OBJ_LIMIT)  * 0.2;
-    if (free_min < FREE_MIN) {
-	do_heap_free = heaps_used * HEAP_OBJ_LIMIT;
-        free_min = FREE_MIN;
+    while (i < heaps_sweep_inc && heaps_sweep_index < heaps_used) {
+	if (slot_sweep(objspace, &heaps[heaps_sweep_index])) {
+	    i++;
+	}
+	heaps_sweep_index++;
     }
+}
 
-    freelist = 0;
-    final_list = deferred_final_list;
-    deferred_final_list = 0;
-    for (i = 0; i < heaps_used; i++) {
-	int n = 0;
-	RVALUE *free = freelist;
-	RVALUE *final = final_list;
+void
+thread_local_sweep_increment(rb_objspace_t *objspace, rb_thread_t *th) {
+    size_t i = 0;
 
-	p = heaps[i].slot; pend = p + heaps[i].limit;
-	while (p < pend) {
-	    if (!(p->as.basic.flags & FL_MARK)) {
-		if (p->as.basic.flags) {
-		    obj_free(objspace, (VALUE)p);
-		}
-		if (need_call_final && FL_TEST(p, FL_FINALIZE)) {
-		    p->as.free.flags = FL_MARK; /* remain marked */
-		    p->as.free.next = final_list;
-		    final_list = p;
-		}
-		else {
-                    VALGRIND_MAKE_MEM_UNDEFINED((void*)p, sizeof(RVALUE));
-		    p->as.free.flags = 0;
-		    p->as.free.next = freelist;
-		    freelist = p;
-		}
-		n++;
-	    }
-	    else if (RBASIC(p)->flags == FL_MARK) {
-		/* objects to be finalized */
-		/* do nothing remain marked */
-	    }
-	    else {
-		RBASIC(p)->flags &= ~FL_MARK;
-		live++;
-	    }
-	    p++;
-	}
-	if (n == heaps[i].limit && freed > do_heap_free) {
-	    RVALUE *pp;
+    while (!th->free_cache && th->sweep_index < th->slots_used) {
+	struct heaps_slot *sweep_slot = thread_sweep_slot(th);
 
-	    heaps[i].limit = 0;
-	    for (pp = final_list; pp != final; pp = pp->as.free.next) {
-		p->as.free.flags |= FL_SINGLETON; /* freeing page mark */
-	    }
-	    freelist = free;	/* cancel this page from freelist */
+	slot_sweep(objspace, sweep_slot);
+
+	if (!sweep_slot->limit) {
+	    sweep_slot->color = WHITE;
+	    MEMMOVE(&th->local_slots[th->sweep_index], (&th->local_slots[th->sweep_index])+1, VALUE, th->slots_used - th->sweep_index - 1);
+	    th->slots_used--;
+	    sweep_slot->thread = 0;
+	} else {
+	    th->sweep_index++;
 	}
-	else {
-	    freed += n;
-	}
     }
-    if (malloc_increase > malloc_limit) {
-	malloc_limit += (malloc_increase - malloc_limit) * (double)live / (live + freed);
-	if (malloc_limit < GC_MALLOC_LIMIT) malloc_limit = GC_MALLOC_LIMIT;
+}
+
+//static void
+int
+gc_lazy_sweep(rb_objspace_t *objspace, rb_thread_t *th) {
+    if (heaps_increment(objspace)) {
+	heap_sweep_increment(objspace);
     }
-    malloc_increase = 0;
-    if (freed < free_min) {
-    	set_heaps_increment(objspace);
-	heaps_increment(objspace);
+    else {
+	thread_local_sweep_increment(objspace, th);
+	heap_sweep_increment(objspace);
     }
-    during_gc = 0;
-
-    /* clear finalization list */
-    if (final_list) {
-	deferred_final_list = final_list;
-	return;
+    
+    if (!th->free_cache) {
+	return Qfalse;
     }
-    free_unused_heaps(objspace);
+    
+    return Qtrue;
 }
 
 void
@@ -1439,9 +1483,8 @@
 {
     rb_objspace_t *objspace = &rb_objspace;
     VALGRIND_MAKE_MEM_UNDEFINED((void*)p, sizeof(RVALUE));
+    RANY(p)->color = WHITE;
     RANY(p)->as.free.flags = 0;
-    RANY(p)->as.free.next = freelist;
-    freelist = RANY(p);
 }
 
 static void
@@ -1647,30 +1690,71 @@
 
 void rb_gc_mark_encodings(void);
 
-static int
-garbage_collect(rb_objspace_t *objspace)
+void
+gc_mark_all_clear(rb_objspace_t *objspace)
 {
+    RVALUE *last = 0;
+    size_t i, j;
+    
+    for (i = j = 0; j < heaps_used; i++) {
+	if (heaps[i].color == WHITE && !deferred_final_list) {
+	    if (!last) {
+		last = heaps[i].membase;
+	    }
+	    else {
+		free(heaps[i].membase);
+	    }
+	    heaps_used--;
+	}
+	else {
+	    if (heaps[i].color == GRAY) {
+		RVALUE *p, *pend;
+		p = RANY(heaps[i].slot); pend = p + heaps[i].limit;
+		while (p < pend) {
+		    if (p->color != BLACK) {
+			if (p->as.basic.flags && !FL_TEST(p, FL_FINALIZE)) {
+			    obj_free(objspace, (VALUE)p);
+			    VALGRIND_MAKE_MEM_UNDEFINED((void*)p, sizeof(RVALUE));
+			    p->as.free.flags = 0;
+			}
+		    }
+		    else if (RBASIC(p)->flags != FL_MARK) {
+			p->color = WHITE;
+		    }
+		    p++;
+		}
+	    }
+	    else {
+		heaps[i].color = GRAY;
+	    }
+	    if (i != j) {
+		heaps[j] = heaps[i];
+	    }
+	    j++;
+	}
+    }
+    if (last) {
+	if (last < heaps_freed) {
+	    free(heaps_freed);
+	    heaps_freed = last;
+	}
+	else {
+	    free(last);
+	}
+    }
+}
+
+//static void
+void
+gc_marks(rb_objspace_t *objspace, rb_thread_t *th)
+{
     struct gc_list *list;
-    rb_thread_t *th = GET_THREAD();
+    size_t free_min = 0;
 
-    if (GC_NOTIFY) printf("start garbage_collect()\n");
+    live = 0;
 
-    if (!heaps) {
-	return Qfalse;
-    }
+    gc_mark_all_clear(objspace);
 
-    if (dont_gc || during_gc) {
-	if (!freelist) {
-            if (!heaps_increment(objspace)) {
-                set_heaps_increment(objspace);
-                heaps_increment(objspace);
-            }
-	}
-	return Qtrue;
-    }
-    during_gc++;
-    objspace->count++;
-
     SET_STACK_END;
 
     init_mark_stack(objspace);
@@ -1711,10 +1795,59 @@
 	    gc_mark_rest(objspace);
 	}
     }
+    heaps_sweep_index = 0;
+    heaps_sweep_inc = (heaps_used / 10) + 1;
+    free_min = (heaps_used * HEAP_OBJ_LIMIT)  * 0.2;
+    if (free_min < FREE_MIN) free_min = FREE_MIN;
+    if (free_min > (heaps_used * HEAP_OBJ_LIMIT - live)) {
+	set_heaps_increment(objspace);
+	heaps_sweep_inc = (heaps_used / heaps_inc) + 1;
+    }
+}
 
-    gc_sweep(objspace);
+//static int
+int
+garbage_collect_force(rb_objspace_t *objspace)
+{
+    if (malloc_increase > malloc_limit) {
+	malloc_limit += (malloc_increase - malloc_limit) * (double)live / (heaps_used * HEAP_OBJ_LIMIT);
+	if (malloc_limit < GC_MALLOC_LIMIT) malloc_limit = GC_MALLOC_LIMIT;
+    }
+    malloc_increase = 0;
+    gc_marks(objspace, GET_THREAD());
+    return garbage_collect(objspace);
+}
 
+//static int
+int
+garbage_collect(rb_objspace_t *objspace)
+{
+    rb_thread_t *th = GET_THREAD();
+
+    if (GC_NOTIFY) printf("start garbage_collect()\n");
+
+    if (!heaps) {
+	return Qfalse;
+    }
+
+    if (dont_gc || during_gc) {
+	if (th->free_cache) {
+            if (!heaps_increment(objspace)) {
+                set_heaps_increment(objspace);
+                heaps_increment(objspace);
+            }
+	}
+	return Qtrue;
+    }
+    during_gc++;
+    objspace->count++;
+
+    while (!gc_lazy_sweep(objspace, th)) {
+	gc_marks(objspace, th);
+    }
+
     if (GC_NOTIFY) printf("end garbage_collect()\n");
+    during_gc = 0;
     return Qtrue;
 }
 
@@ -1871,7 +2004,7 @@
     for (i = 0; i < heaps_used; i++) {
 	RVALUE *p, *pend;
 
-	p = heaps[i].slot; pend = p + heaps[i].limit;
+	p = RANY(heaps[i].slot); pend = p + heaps[i].limit;
 	for (;p < pend; p++) {
 	    if (p->as.basic.flags) {
 		switch (BUILTIN_TYPE(p)) {
@@ -2060,7 +2193,6 @@
     if (p) {
 	finalize_list(objspace, p);
     }
-    free_unused_heaps(objspace);
     during_gc = 0;
 }
 
@@ -2085,7 +2217,7 @@
 	deferred_final_list = 0;
 	finalize_list(objspace, p);
 	for (i = 0; i < heaps_used; i++) {
-	    p = heaps[i].slot; pend = p + heaps[i].limit;
+	    p = RANY(heaps[i].slot); pend = p + heaps[i].limit;
 	    while (p < pend) {
 		if (FL_TEST(p, FL_FINALIZE)) {
 		    FL_UNSET(p, FL_FINALIZE);
@@ -2098,12 +2230,13 @@
     }
     /* run data object's finalizers */
     for (i = 0; i < heaps_used; i++) {
-	p = heaps[i].slot; pend = p + heaps[i].limit;
+	p = RANY(heaps[i].slot); pend = p + heaps[i].limit;
 	while (p < pend) {
 	    if (BUILTIN_TYPE(p) == T_DATA &&
 		DATA_PTR(p) && RANY(p)->as.data.dfree &&
 		RANY(p)->as.basic.klass != rb_cThread) {
 		p->as.free.flags = 0;
+		p->color = WHITE;
 		if ((long)RANY(p)->as.data.dfree == -1) {
 		    RUBY_CRITICAL(free(DATA_PTR(p)));
 		}
@@ -2115,6 +2248,7 @@
 	    else if (BUILTIN_TYPE(p) == T_FILE) {
 		if (rb_io_fptr_finalize(RANY(p)->as.file.fptr)) {
 		    p->as.free.flags = 0;
+		    p->color = WHITE;
                     VALGRIND_MAKE_MEM_UNDEFINED((void*)p, sizeof(RVALUE));
 		}
 	    }
@@ -2128,8 +2262,8 @@
 rb_gc(void)
 {
     rb_objspace_t *objspace = &rb_objspace;
+    gc_finalize_deferred(objspace);
     garbage_collect(objspace);
-    gc_finalize_deferred(objspace);
 }
 
 /*
@@ -2293,7 +2427,7 @@
     for (i = 0; i < heaps_used; i++) {
         RVALUE *p, *pend;
 
-        p = heaps[i].slot; pend = p + heaps[i].limit;
+        p = RANY(heaps[i].slot); pend = p + heaps[i].limit;
         for (;p < pend; p++) {
             if (p->as.basic.flags) {
                 counts[BUILTIN_TYPE(p)]++;
Index: gc.h
===================================================================
--- gc.h	(リビジョン 16421)
+++ gc.h	(作業コピー)
@@ -1,4 +1,16 @@
+#define WHITE 0x00
+#define BLACK 0x01
+#define GRAY  0x02
 
+struct heaps_slot {
+    void *membase;
+    VALUE slot;
+    int limit;
+    char color;  // white : garbage, black : used, gray : not sweep
+    VALUE thread;
+    size_t *local_slot;
+};
+
 #ifndef RUBY_GC_H
 #define RUBY_GC_H 1
 
Index: vm.c
===================================================================
--- vm.c	(リビジョン 16421)
+++ vm.c	(作業コピー)
@@ -1561,6 +1561,7 @@
 thread_free(void *ptr)
 {
     rb_thread_t *th;
+    int i;
     RUBY_FREE_ENTER("thread");
 
     if (ptr) {
@@ -1585,6 +1586,12 @@
 	    }
 	}
 #endif
+	th->free_cache = 0;
+	for(i = 0; i > th->slots_used; i++) {
+	    ((struct heaps_slot *)th->local_slots[i])->color = BLACK;
+	    ((struct heaps_slot *)th->local_slots[i])->thread = 0;
+	}
+	ruby_xfree(th->local_slots);
 
 	if (th->vm->main_thread == th) {
 	    RUBY_GC_INFO("main thread\n");
@@ -1647,6 +1654,10 @@
 	}
 
 	mark_event_hooks(th->event_hooks);
+	//lazy sweep
+	th->free_cache = 0;
+	th->freed = 0;
+	th->sweep_index = 0;
     }
 
     RUBY_MARK_LEAVE("thread");
